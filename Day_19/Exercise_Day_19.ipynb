{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c019fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def counter(file_name):\n",
    "    num_words = 0\n",
    "    num_lines = 0\n",
    "\n",
    "    with open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(os.linesep)\n",
    "            words_list = line.split()\n",
    "            num_lines += 1\n",
    "            num_words += len(words_list)\n",
    "\n",
    "    print(\"Number of words: \", num_words)\n",
    "    print(\"Number of lines: \", num_lines)\n",
    "\n",
    "# Question 1(a) Read obama_speech.txt\n",
    "print(\"Obama Speech \")\n",
    "counter('./data/obama_speech.txt')\n",
    "\n",
    "# Question1(b) Read michelle_obama_speech.txt\n",
    "print(\"\\nMichelle Obama Speech\")\n",
    "counter(\"./data/michelle_obama_speech.txt\")\n",
    "\n",
    "# Question1(c) Read donald_speech.txt\n",
    "print(\"\\nDonald Speech \")\n",
    "counter(\"./data/donald_speech.txt\")\n",
    "\n",
    "# Question1(d) Read melina_trump_speech.txt\n",
    "print(\"\\nMelania Trump Speech\")\n",
    "counter(\"./data/melina_trump_speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def most_spoken_languages(filename,value):\n",
    "\n",
    "    languages_lst = []\n",
    "    for i in json.load(open(filename,encoding=\"UTF8\")):\n",
    "        languages_lst.extend(i['languages'])\n",
    "\n",
    "    #print('Total number of languages is',len(set(languages_lst)))\n",
    "    languages_dict = {}\n",
    "    for j in languages_lst:\n",
    "        if j in languages_dict:\n",
    "            languages_dict[j] += 1\n",
    "        else:\n",
    "            languages_dict[j] = 1\n",
    "    # sort languages dictionary by value to get top language\n",
    "    sorted_language_dict= dict(sorted(languages_dict.items(),key=lambda x:x[1],reverse=True))\n",
    "\n",
    "    # get top languages list by passing value\n",
    "    top_language = list(sorted_language_dict.items())[:value]\n",
    "    print(\"top {} most spoken languages from the data\".format(value))\n",
    "    return [(i[1],i[0]) for  i in top_language]\n",
    "\n",
    "print(most_spoken_languages('./data/countries_data.json',10))\n",
    "print(most_spoken_languages('./data/countries_data.json',4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdb8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_populated_countries(filename,value):\n",
    "    populations = {}\n",
    "    for i in json.load(open(filename,encoding=\"UTF8\")):\n",
    "        populations[i[\"name\"]] = i[\"population\"]\n",
    "    sorted_populations_dict= dict(sorted(populations.items(),key=lambda x:x[1],reverse=True))\n",
    "    top_most_populated_countries = []\n",
    "    for l in list(sorted_populations_dict.items())[:value]:\n",
    "        top_most_populated_countries.append({'country':l[0],'populations':l[1]})\n",
    "\n",
    "    print(\"top {} most populated countries in the world\".format(value))\n",
    "    return top_most_populated_countries\n",
    "\n",
    "print(most_populated_countries('./data/countries_data.json',10))\n",
    "print(most_populated_countries('./data/countries_data.json',4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(fname):\n",
    "    word_list = []\n",
    "    with open(fname, \"r\") as file:\n",
    "        for line in file:\n",
    "            for word in line.split():\n",
    "                word_list.append(word)\n",
    "    # change each email address in the list  to unique using set data type\n",
    "    word_list = list(set(word_list))\n",
    "\n",
    "    email_list = []\n",
    "    for word in word_list:\n",
    "        if re.fullmatch(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", word):\n",
    "            email_list.append(word)\n",
    "    return email_list\n",
    "\n",
    "print('Email List:')\n",
    "print(extract_emails(\"data\\email_exchanges_big.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eeeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def find_most_common_words(filename, value):\n",
    "    text = open(filename).read()\n",
    "    split_text = text.split()\n",
    "    my_counter = [(i[1], i[0]) for i in Counter(split_text).most_common()]\n",
    "\n",
    "    return my_counter[:value]\n",
    "\n",
    "print(find_most_common_words('data\\donald_speech.txt', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5595af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6(a)\n",
    "print(\"Obama Speech\")\n",
    "print(find_most_common_words('./data/obama_speech.txt',10))\n",
    "\n",
    "# Q6(b)\n",
    "print(\"Michelle Speech\")\n",
    "print(find_most_common_words('./data/michelle_obama_speech.txt',10))\n",
    "\n",
    "# Q6(c)\n",
    "print(\"Donald Speech\")\n",
    "print(find_most_common_words('./data/donald_speech.txt',10))\n",
    "\n",
    "# Q6(d)\n",
    "print(\"Melina Speech\")\n",
    "print(find_most_common_words('./data/melina_trump_speech.txt',10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from data.stop_words import stop_words\n",
    "import math\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\[.*?]\", \"\", text)\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(\"<.*?>+\", \"\", text)\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_support_words(text):\n",
    "    return [word for word in text.split() if word not in stop_words]\n",
    "\n",
    "\n",
    "def read_file(fname):\n",
    "    try:\n",
    "        with open(fname, \"r\", encoding=\"UTF8\") as f:\n",
    "            data = remove_support_words(clean_text(f.read()))\n",
    "        return data\n",
    "    except IOError:\n",
    "        print(\"Error opening or reading input file: \", fname)\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "translation_table = str.maketrans(\n",
    "    string.punctuation + string.ascii_uppercase,\n",
    "    \" \" * len(string.punctuation) + string.ascii_lowercase,\n",
    ")\n",
    "\n",
    "def count_frequency(word_list):\n",
    "    D = {}\n",
    "\n",
    "    for new_word in word_list:\n",
    "\n",
    "        if new_word in D:\n",
    "            D[new_word] = D[new_word] + 1\n",
    "\n",
    "        else:\n",
    "            D[new_word] = 1\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "def word_frequencies_for_file(filename):\n",
    "    word_list = read_file(filename)\n",
    "    freq_mapping = count_frequency(word_list)\n",
    "\n",
    "    print(\n",
    "        \"File\",\n",
    "        filename,\n",
    "        \":\",\n",
    "    )\n",
    "    print(len(freq_mapping), \"distinct words\")\n",
    "\n",
    "    return freq_mapping\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def dotProduct(D1, D2):\n",
    "    Sum = 0.0\n",
    "\n",
    "    for key in D1:\n",
    "\n",
    "        if key in D2:\n",
    "            Sum += D1[key] * D2[key]\n",
    "\n",
    "    return Sum\n",
    "\n",
    "\n",
    "# noinspection PyPep8Naming\n",
    "def vector_angle(D1, D2):\n",
    "    numerator = dotProduct(D1, D2)\n",
    "    denominator = math.sqrt(dotProduct(D1, D1) * dotProduct(D2, D2))\n",
    "\n",
    "    return math.acos(numerator / denominator)\n",
    "\n",
    "\n",
    "def documentSimilarity(filename_1, filename_2):\n",
    "    sorted_word_list_1 = word_frequencies_for_file(filename_1)\n",
    "    sorted_word_list_2 = word_frequencies_for_file(filename_2)\n",
    "    distance = (vector_angle(sorted_word_list_1, sorted_word_list_2) * 180) / math.pi\n",
    "\n",
    "    print(\"The distance between the documents is: % 0.2f (degrees)\" % distance)\n",
    "\n",
    "documentSimilarity(\"data\\michelle_obama_speech.txt\", \"data\\melina_trump_speech.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
